{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1: Data Preprocessing & Feature Engineering\n",
        "\n",
        "**Objective:** Prepare the Amazon products dataset for machine learning modeling\n",
        "\n",
        "---\n",
        "\n",
        "## Steps:\n",
        "1. Load and clean data\n",
        "2. Handle missing values\n",
        "3. Feature engineering\n",
        "4. Encode categorical variables\n",
        "5. Scale numerical features\n",
        "6. Save preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "import pickle\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original dataset shape: (42675, 17)\n",
            "\n",
            "Columns: ['product_title', 'product_rating', 'total_reviews', 'purchased_last_month', 'discounted_price', 'original_price', 'is_best_seller', 'is_sponsored', 'has_coupon', 'buy_box_availability', 'delivery_date', 'sustainability_tags', 'product_image_url', 'product_page_url', 'data_collected_at', 'product_category', 'discount_percentage']\n",
            "\n",
            "Missing values:\n",
            "product_title               0\n",
            "product_rating           1024\n",
            "total_reviews            1024\n",
            "purchased_last_month    10511\n",
            "discounted_price         2062\n",
            "original_price           2062\n",
            "is_best_seller              0\n",
            "is_sponsored                0\n",
            "has_coupon                  0\n",
            "buy_box_availability    14653\n",
            "delivery_date           11983\n",
            "sustainability_tags     39267\n",
            "product_image_url           0\n",
            "product_page_url         2069\n",
            "data_collected_at           0\n",
            "product_category            0\n",
            "discount_percentage      2062\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv('amazon_products_sales_data_cleaned.csv')\n",
        "\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Handle Target Variable Missing Values\n",
        "\n",
        "Since our target is `purchased_last_month`, we need to remove rows where it's missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows before removing target nulls: 42675\n",
            "Rows after removing target nulls: 32164\n",
            "Rows removed: 10511\n"
          ]
        }
      ],
      "source": [
        "# Remove rows where target variable is missing\n",
        "print(f\"Rows before removing target nulls: {len(df)}\")\n",
        "df = df.dropna(subset=['purchased_last_month'])\n",
        "print(f\"Rows after removing target nulls: {len(df)}\")\n",
        "print(f\"Rows removed: {42675 - len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Select and Clean Features\n",
        "\n",
        "Remove unnecessary columns (URLs, timestamps) and focus on predictive features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remaining columns: ['product_rating', 'total_reviews', 'purchased_last_month', 'discounted_price', 'original_price', 'is_best_seller', 'is_sponsored', 'has_coupon', 'buy_box_availability', 'sustainability_tags', 'product_category', 'discount_percentage']\n",
            "\n",
            "Dataset shape: (32164, 12)\n"
          ]
        }
      ],
      "source": [
        "# Drop unnecessary columns\n",
        "columns_to_drop = [\n",
        "    'product_title',  # Text - would need NLP processing\n",
        "    'product_image_url',  # Not predictive\n",
        "    'product_page_url',  # Not predictive\n",
        "    'data_collected_at',  # Timestamp - all same date\n",
        "    'delivery_date'  # Too many missing values and not reliable\n",
        "]\n",
        "\n",
        "df = df.drop(columns=columns_to_drop)\n",
        "print(f\"Remaining columns: {df.columns.tolist()}\")\n",
        "print(f\"\\nDataset shape: {df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Handle Missing Values in Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values:\n",
            "                      Missing_Count  Missing_Percentage\n",
            "sustainability_tags           29422               91.47\n",
            "buy_box_availability           8662               26.93\n",
            "discounted_price               1860                5.78\n",
            "original_price                 1860                5.78\n",
            "discount_percentage            1860                5.78\n",
            "product_rating                   76                0.24\n",
            "total_reviews                    76                0.24\n"
          ]
        }
      ],
      "source": [
        "# Check missing values\n",
        "missing = df.isnull().sum()\n",
        "missing_pct = (missing / len(df) * 100).round(2)\n",
        "missing_df = pd.DataFrame({'Missing_Count': missing, 'Missing_Percentage': missing_pct})\n",
        "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
        "print(\"Missing values:\")\n",
        "print(missing_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing values after handling:\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# Handle sustainability_tags (too many missing - drop or create flag)\n",
        "df['has_sustainability'] = df['sustainability_tags'].notna().astype(int)\n",
        "df = df.drop('sustainability_tags', axis=1)\n",
        "\n",
        "# Handle buy_box_availability (many missing - create flag)\n",
        "df['has_buy_box_info'] = df['buy_box_availability'].notna().astype(int)\n",
        "df['buy_box_availability'] = df['buy_box_availability'].fillna('Unknown')\n",
        "\n",
        "# Handle numerical features with missing values\n",
        "# For rating and reviews - fill with median (neutral approach)\n",
        "df['product_rating'] = df['product_rating'].fillna(df['product_rating'].median())\n",
        "df['total_reviews'] = df['total_reviews'].fillna(df['total_reviews'].median())\n",
        "\n",
        "# For price features - fill with median\n",
        "df['discounted_price'] = df['discounted_price'].fillna(df['discounted_price'].median())\n",
        "df['original_price'] = df['original_price'].fillna(df['original_price'].median())\n",
        "df['discount_percentage'] = df['discount_percentage'].fillna(0)  # No discount if missing\n",
        "\n",
        "print(\"\\nMissing values after handling:\")\n",
        "print(df.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Engineered features created!\n",
            "\n",
            "New feature columns: ['is_best_seller', 'is_sponsored', 'has_coupon', 'buy_box_availability', 'product_category', 'has_sustainability', 'has_buy_box_info', 'discount_amount', 'price_ratio', 'rating_review_interaction', 'log_total_reviews', 'log_purchased_last_month']\n"
          ]
        }
      ],
      "source": [
        "# Create new features\n",
        "\n",
        "# Price-based features\n",
        "df['discount_amount'] = df['original_price'] - df['discounted_price']\n",
        "df['price_ratio'] = df['discounted_price'] / (df['original_price'] + 1)  # +1 to avoid division by zero\n",
        "\n",
        "# Rating and review features\n",
        "df['rating_review_interaction'] = df['product_rating'] * np.log1p(df['total_reviews'])\n",
        "df['log_total_reviews'] = np.log1p(df['total_reviews'])\n",
        "\n",
        "# Log transform the target variable to reduce skewness\n",
        "df['log_purchased_last_month'] = np.log1p(df['purchased_last_month'])\n",
        "\n",
        "print(\"Engineered features created!\")\n",
        "print(f\"\\nNew feature columns: {[col for col in df.columns if col not in ['product_rating', 'total_reviews', 'purchased_last_month', 'discounted_price', 'original_price', 'discount_percentage']]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Encode Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Number of unique categories: 15\n",
            "Category value counts:\n",
            "product_category\n",
            "Laptops                6634\n",
            "Phones                 5806\n",
            "Other Electronics      5425\n",
            "Power & Batteries      2783\n",
            "Cameras                2599\n",
            "Chargers & Cables      1546\n",
            "TV & Display           1373\n",
            "Headphones              993\n",
            "Speakers                959\n",
            "Storage                 958\n",
            "Printers & Scanners     876\n",
            "Networking              846\n",
            "Gaming                  666\n",
            "Wearables               447\n",
            "Smart Home              253\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Final dataset shape: (32164, 31)\n",
            "\n",
            "Columns: ['product_rating', 'total_reviews', 'purchased_last_month', 'discounted_price', 'original_price', 'is_best_seller', 'is_sponsored', 'has_coupon', 'discount_percentage', 'has_sustainability', 'has_buy_box_info', 'discount_amount', 'price_ratio', 'rating_review_interaction', 'log_total_reviews', 'log_purchased_last_month', 'has_add_to_cart', 'category_Chargers & Cables', 'category_Gaming', 'category_Headphones', 'category_Laptops', 'category_Networking', 'category_Other Electronics', 'category_Phones', 'category_Power & Batteries', 'category_Printers & Scanners', 'category_Smart Home', 'category_Speakers', 'category_Storage', 'category_TV & Display', 'category_Wearables']\n"
          ]
        }
      ],
      "source": [
        "# Binary encoding for badge/promotional features\n",
        "df['is_best_seller'] = (df['is_best_seller'] == 'Best Seller').astype(int)\n",
        "df['is_sponsored'] = (df['is_sponsored'] == 'Sponsored').astype(int)\n",
        "df['has_coupon'] = (df['has_coupon'] != 'No Coupon').astype(int)\n",
        "\n",
        "# Encode buy_box_availability\n",
        "df['has_add_to_cart'] = (df['buy_box_availability'] == 'Add to cart').astype(int)\n",
        "df = df.drop('buy_box_availability', axis=1)\n",
        "\n",
        "# One-hot encode product_category (if not too many categories)\n",
        "print(f\"\\nNumber of unique categories: {df['product_category'].nunique()}\")\n",
        "print(f\"Category value counts:\\n{df['product_category'].value_counts()}\")\n",
        "\n",
        "# One-hot encode categories\n",
        "category_dummies = pd.get_dummies(df['product_category'], prefix='category', drop_first=True)\n",
        "df = pd.concat([df, category_dummies], axis=1)\n",
        "df = df.drop('product_category', axis=1)\n",
        "\n",
        "print(f\"\\nFinal dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Prepare Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features shape: (32164, 29)\n",
            "Target (original) shape: (32164,)\n",
            "Target (log) shape: (32164,)\n",
            "\n",
            "Feature columns: ['product_rating', 'total_reviews', 'discounted_price', 'original_price', 'is_best_seller', 'is_sponsored', 'has_coupon', 'discount_percentage', 'has_sustainability', 'has_buy_box_info', 'discount_amount', 'price_ratio', 'rating_review_interaction', 'log_total_reviews', 'has_add_to_cart', 'category_Chargers & Cables', 'category_Gaming', 'category_Headphones', 'category_Laptops', 'category_Networking', 'category_Other Electronics', 'category_Phones', 'category_Power & Batteries', 'category_Printers & Scanners', 'category_Smart Home', 'category_Speakers', 'category_Storage', 'category_TV & Display', 'category_Wearables']\n"
          ]
        }
      ],
      "source": [
        "# Separate features and targets\n",
        "# We'll keep both original and log-transformed target for different models\n",
        "\n",
        "# Original target\n",
        "y_original = df['purchased_last_month'].copy()\n",
        "\n",
        "# Log-transformed target (better for models sensitive to outliers)\n",
        "y_log = df['log_purchased_last_month'].copy()\n",
        "\n",
        "# Features (drop both target variables)\n",
        "X = df.drop(['purchased_last_month', 'log_purchased_last_month'], axis=1)\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target (original) shape: {y_original.shape}\")\n",
        "print(f\"Target (log) shape: {y_log.shape}\")\n",
        "print(f\"\\nFeature columns: {X.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 25731\n",
            "Test set size: 6433\n",
            "\n",
            "Training set percentage: 80.0%\n",
            "Test set percentage: 20.0%\n"
          ]
        }
      ],
      "source": [
        "# Split data (80/20)\n",
        "X_train, X_test, y_train_original, y_test_original = train_test_split(\n",
        "    X, y_original, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Also split for log-transformed target\n",
        "_, _, y_train_log, y_test_log = train_test_split(\n",
        "    X, y_log, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "print(f\"\\nTraining set percentage: {X_train.shape[0] / len(X) * 100:.1f}%\")\n",
        "print(f\"Test set percentage: {X_test.shape[0] / len(X) * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Scale Numerical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features scaled successfully!\n",
            "\n",
            "Scaled training data shape: (25731, 29)\n",
            "Scaled test data shape: (6433, 29)\n"
          ]
        }
      ],
      "source": [
        "# Fit scaler on training data only\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame to keep column names\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "print(\"Features scaled successfully!\")\n",
        "print(f\"\\nScaled training data shape: {X_train_scaled.shape}\")\n",
        "print(f\"Scaled test data shape: {X_test_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Preprocessed Data and Scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ All preprocessed data saved successfully!\n",
            "\n",
            "Files created:\n",
            "  - X_train_scaled.csv (scaled features for linear models)\n",
            "  - X_test_scaled.csv\n",
            "  - X_train.csv (unscaled features for tree models)\n",
            "  - X_test.csv\n",
            "  - y_train_original.csv\n",
            "  - y_test_original.csv\n",
            "  - y_train_log.csv\n",
            "  - y_test_log.csv\n",
            "  - scaler.pkl\n",
            "  - feature_names.pkl\n"
          ]
        }
      ],
      "source": [
        "# Save preprocessed data\n",
        "X_train_scaled.to_csv('X_train_scaled.csv', index=False)\n",
        "X_test_scaled.to_csv('X_test_scaled.csv', index=False)\n",
        "\n",
        "# Save original (unscaled) data as well for tree-based models\n",
        "X_train.to_csv('X_train.csv', index=False)\n",
        "X_test.to_csv('X_test.csv', index=False)\n",
        "\n",
        "# Save targets\n",
        "y_train_original.to_csv('y_train_original.csv', index=False, header=['purchased_last_month'])\n",
        "y_test_original.to_csv('y_test_original.csv', index=False, header=['purchased_last_month'])\n",
        "y_train_log.to_csv('y_train_log.csv', index=False, header=['log_purchased_last_month'])\n",
        "y_test_log.to_csv('y_test_log.csv', index=False, header=['log_purchased_last_month'])\n",
        "\n",
        "# Save scaler for future use\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "# Save feature names\n",
        "feature_names = X_train.columns.tolist()\n",
        "with open('feature_names.pkl', 'wb') as f:\n",
        "    pickle.dump(feature_names, f)\n",
        "\n",
        "print(\"✓ All preprocessed data saved successfully!\")\n",
        "print(\"\\nFiles created:\")\n",
        "print(\"  - X_train_scaled.csv (scaled features for linear models)\")\n",
        "print(\"  - X_test_scaled.csv\")\n",
        "print(\"  - X_train.csv (unscaled features for tree models)\")\n",
        "print(\"  - X_test.csv\")\n",
        "print(\"  - y_train_original.csv\")\n",
        "print(\"  - y_test_original.csv\")\n",
        "print(\"  - y_train_log.csv\")\n",
        "print(\"  - y_test_log.csv\")\n",
        "print(\"  - scaler.pkl\")\n",
        "print(\"  - feature_names.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "PREPROCESSING SUMMARY\n",
            "============================================================\n",
            "\n",
            "Original dataset: 42,675 rows × 17 columns\n",
            "After removing missing targets: 32164 rows\n",
            "Final feature count: 29 features\n",
            "\n",
            "Training samples: 25731\n",
            "Test samples: 6433\n",
            "\n",
            "Target variable (original):\n",
            "  Train - Mean: 1312.64, Median: 200.00\n",
            "  Test  - Mean: 1217.77, Median: 200.00\n",
            "\n",
            "Target variable (log-transformed):\n",
            "  Train - Mean: 5.40, Median: 5.30\n",
            "  Test  - Mean: 5.39, Median: 5.30\n",
            "\n",
            "✓ Data is ready for model training!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Display summary\n",
        "print(\"=\" * 60)\n",
        "print(\"PREPROCESSING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nOriginal dataset: 42,675 rows × 17 columns\")\n",
        "print(f\"After removing missing targets: {len(X)} rows\")\n",
        "print(f\"Final feature count: {X_train.shape[1]} features\")\n",
        "print(f\"\\nTraining samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples: {X_test.shape[0]}\")\n",
        "print(f\"\\nTarget variable (original):\")\n",
        "print(f\"  Train - Mean: {y_train_original.mean():.2f}, Median: {y_train_original.median():.2f}\")\n",
        "print(f\"  Test  - Mean: {y_test_original.mean():.2f}, Median: {y_test_original.median():.2f}\")\n",
        "print(f\"\\nTarget variable (log-transformed):\")\n",
        "print(f\"  Train - Mean: {y_train_log.mean():.2f}, Median: {y_train_log.median():.2f}\")\n",
        "print(f\"  Test  - Mean: {y_test_log.mean():.2f}, Median: {y_test_log.median():.2f}\")\n",
        "print(f\"\\n✓ Data is ready for model training!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Proceed to **Phase 2: Model Training** to train multiple regression models using this preprocessed data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (dsproj)",
      "language": "python",
      "name": "dsproj"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
