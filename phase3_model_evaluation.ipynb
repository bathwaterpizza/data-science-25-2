{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 3: Model Evaluation & Comparison\n",
        "\n",
        "**Objective:** Evaluate all trained models and compare their performance\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation Metrics:\n",
        "- **RMSE** (Root Mean Squared Error) - penalizes large errors\n",
        "- **MAE** (Mean Absolute Error) - average absolute error\n",
        "- **R¬≤ Score** - proportion of variance explained (0-1, higher is better)\n",
        "- **MAPE** (Mean Absolute Percentage Error) - percentage error\n",
        "\n",
        "## Visualizations:\n",
        "- Model comparison table\n",
        "- Actual vs Predicted plots\n",
        "- Residual plots\n",
        "- Feature importance (for tree models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test data\n",
        "X_test_scaled = pd.read_csv('X_test_scaled.csv')\n",
        "X_test = pd.read_csv('X_test.csv')\n",
        "y_test_original = pd.read_csv('y_test_original.csv')['purchased_last_month']\n",
        "y_test_log = pd.read_csv('y_test_log.csv')['log_purchased_last_month']\n",
        "\n",
        "print(\"Test data loaded successfully!\")\n",
        "print(f\"Test set size: {len(y_test_original)}\")\n",
        "print(f\"Number of features: {X_test.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load All Trained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load models\n",
        "models = {}\n",
        "model_names = ['linear_regression', 'ridge', 'lasso', 'elasticnet', \n",
        "               'random_forest', 'xgboost', 'gradient_boosting']\n",
        "\n",
        "for model_name in model_names:\n",
        "    with open(f'models/{model_name}.pkl', 'rb') as f:\n",
        "        models[model_name] = pickle.load(f)\n",
        "\n",
        "# Load metadata\n",
        "with open('models/model_metadata.pkl', 'rb') as f:\n",
        "    model_metadata = pickle.load(f)\n",
        "\n",
        "with open('models/training_times.pkl', 'rb') as f:\n",
        "    training_times = pickle.load(f)\n",
        "\n",
        "print(f\"‚úì Loaded {len(models)} models successfully!\")\n",
        "print(f\"\\nModels loaded: {list(models.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionary to store predictions\n",
        "predictions = {}\n",
        "\n",
        "print(\"Making predictions...\\n\")\n",
        "\n",
        "# For linear models: use scaled data and convert back from log space\n",
        "for model_name in model_metadata['linear_models']:\n",
        "    model = models[model_name]\n",
        "    # Predict in log space\n",
        "    y_pred_log = model.predict(X_test_scaled)\n",
        "    # Convert back to original space\n",
        "    y_pred = np.expm1(y_pred_log)  # inverse of log1p\n",
        "    predictions[model_name] = y_pred\n",
        "    print(f\"‚úì {model_name}: predictions made (log-transformed then converted)\")\n",
        "\n",
        "print()\n",
        "\n",
        "# For tree models: use unscaled data, predictions already in original space\n",
        "for model_name in model_metadata['tree_models']:\n",
        "    model = models[model_name]\n",
        "    y_pred = model.predict(X_test)\n",
        "    predictions[model_name] = y_pred\n",
        "    print(f\"‚úì {model_name}: predictions made (original space)\")\n",
        "\n",
        "print(\"\\n‚úì All predictions completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Calculate Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate MAPE\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    # Avoid division by zero\n",
        "    mask = y_true != 0\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "# Calculate metrics for all models\n",
        "results = []\n",
        "\n",
        "for model_name, y_pred in predictions.items():\n",
        "    # Ensure no negative predictions\n",
        "    y_pred = np.maximum(y_pred, 0)\n",
        "    \n",
        "    rmse = np.sqrt(mean_squared_error(y_test_original, y_pred))\n",
        "    mae = mean_absolute_error(y_test_original, y_pred)\n",
        "    r2 = r2_score(y_test_original, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_test_original.values, y_pred)\n",
        "    \n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R¬≤': r2,\n",
        "        'MAPE (%)': mape,\n",
        "        'Training Time (s)': training_times[model_name]\n",
        "    })\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('RMSE')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualize Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Sort for better visualization\n",
        "results_sorted = results_df.sort_values('RMSE', ascending=False)\n",
        "\n",
        "# 1. RMSE Comparison\n",
        "axes[0, 0].barh(results_sorted['Model'], results_sorted['RMSE'], color='coral')\n",
        "axes[0, 0].set_xlabel('RMSE (Lower is Better)')\n",
        "axes[0, 0].set_title('Root Mean Squared Error by Model')\n",
        "axes[0, 0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# 2. MAE Comparison\n",
        "results_sorted_mae = results_df.sort_values('MAE', ascending=False)\n",
        "axes[0, 1].barh(results_sorted_mae['Model'], results_sorted_mae['MAE'], color='skyblue')\n",
        "axes[0, 1].set_xlabel('MAE (Lower is Better)')\n",
        "axes[0, 1].set_title('Mean Absolute Error by Model')\n",
        "axes[0, 1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# 3. R¬≤ Score Comparison\n",
        "results_sorted_r2 = results_df.sort_values('R¬≤')\n",
        "colors_r2 = ['green' if x > 0.5 else 'orange' if x > 0.3 else 'red' for x in results_sorted_r2['R¬≤']]\n",
        "axes[1, 0].barh(results_sorted_r2['Model'], results_sorted_r2['R¬≤'], color=colors_r2)\n",
        "axes[1, 0].set_xlabel('R¬≤ Score (Higher is Better)')\n",
        "axes[1, 0].set_title('R¬≤ Score by Model')\n",
        "axes[1, 0].axvline(x=0.5, color='black', linestyle='--', alpha=0.5, label='Good threshold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# 4. MAPE Comparison\n",
        "results_sorted_mape = results_df.sort_values('MAPE (%)', ascending=False)\n",
        "axes[1, 1].barh(results_sorted_mape['Model'], results_sorted_mape['MAPE (%)'], color='lightgreen')\n",
        "axes[1, 1].set_xlabel('MAPE % (Lower is Better)')\n",
        "axes[1, 1].set_title('Mean Absolute Percentage Error by Model')\n",
        "axes[1, 1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì Comparison plot saved as 'model_comparison.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Best Model Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify best model based on RMSE\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_model_metrics = results_df.iloc[0]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"BEST PERFORMING MODEL\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nModel: {best_model_name.upper()}\")\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  RMSE: {best_model_metrics['RMSE']:.2f}\")\n",
        "print(f\"  MAE: {best_model_metrics['MAE']:.2f}\")\n",
        "print(f\"  R¬≤ Score: {best_model_metrics['R¬≤']:.4f}\")\n",
        "print(f\"  MAPE: {best_model_metrics['MAPE (%)']:.2f}%\")\n",
        "print(f\"  Training Time: {best_model_metrics['Training Time (s)']:.2f} seconds\")\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Actual vs Predicted Plots for Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions from best model\n",
        "best_predictions = predictions[best_model_name]\n",
        "best_predictions = np.maximum(best_predictions, 0)  # Ensure no negative predictions\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# 1. Scatter plot: Actual vs Predicted\n",
        "axes[0].scatter(y_test_original, best_predictions, alpha=0.5, s=20)\n",
        "axes[0].plot([y_test_original.min(), y_test_original.max()], \n",
        "             [y_test_original.min(), y_test_original.max()], \n",
        "             'r--', lw=2, label='Perfect Prediction')\n",
        "axes[0].set_xlabel('Actual Values')\n",
        "axes[0].set_ylabel('Predicted Values')\n",
        "axes[0].set_title(f'Actual vs Predicted - {best_model_name.upper()}')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Residual plot\n",
        "residuals = y_test_original - best_predictions\n",
        "axes[1].scatter(best_predictions, residuals, alpha=0.5, s=20)\n",
        "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "axes[1].set_xlabel('Predicted Values')\n",
        "axes[1].set_ylabel('Residuals (Actual - Predicted)')\n",
        "axes[1].set_title(f'Residual Plot - {best_model_name.upper()}')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{best_model_name}_predictions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úì Prediction plots saved as '{best_model_name}_predictions.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Feature Importance (for Tree Models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if best model is a tree model\n",
        "if best_model_name in model_metadata['tree_models']:\n",
        "    # Get feature importance\n",
        "    feature_importance = models[best_model_name].feature_importances_\n",
        "    feature_names = X_test.columns\n",
        "    \n",
        "    # Create DataFrame\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importance\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "    \n",
        "    # Display top 20 features\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TOP 20 MOST IMPORTANT FEATURES\")\n",
        "    print(\"=\" * 60)\n",
        "    print(importance_df.head(20).to_string(index=False))\n",
        "    \n",
        "    # Visualize top 15\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_features = importance_df.head(15)\n",
        "    plt.barh(range(len(top_features)), top_features['Importance'], color='steelblue')\n",
        "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.title(f'Top 15 Feature Importance - {best_model_name.upper()}')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úì Feature importance plot saved as 'feature_importance.png'\")\n",
        "else:\n",
        "    print(f\"\\nNote: Feature importance not available for {best_model_name} (linear model)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Prediction Distribution Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare distribution of actual vs predicted values\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# 1. Histogram comparison\n",
        "axes[0].hist(y_test_original, bins=50, alpha=0.5, label='Actual', color='blue', edgecolor='black')\n",
        "axes[0].hist(best_predictions, bins=50, alpha=0.5, label='Predicted', color='red', edgecolor='black')\n",
        "axes[0].set_xlabel('Purchased Last Month')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Distribution: Actual vs Predicted')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Box plot comparison\n",
        "data_to_plot = [y_test_original, best_predictions]\n",
        "axes[1].boxplot(data_to_plot, labels=['Actual', 'Predicted'])\n",
        "axes[1].set_ylabel('Purchased Last Month')\n",
        "axes[1].set_title('Box Plot: Actual vs Predicted')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('distribution_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì Distribution comparison saved as 'distribution_comparison.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze prediction errors\n",
        "errors = y_test_original - best_predictions\n",
        "abs_errors = np.abs(errors)\n",
        "pct_errors = (abs_errors / y_test_original) * 100\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ERROR ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nError Statistics:\")\n",
        "print(f\"  Mean Error: {errors.mean():.2f}\")\n",
        "print(f\"  Std Error: {errors.std():.2f}\")\n",
        "print(f\"  Mean Absolute Error: {abs_errors.mean():.2f}\")\n",
        "print(f\"  Median Absolute Error: {abs_errors.median():.2f}\")\n",
        "print(f\"  Mean Percentage Error: {pct_errors.mean():.2f}%\")\n",
        "print(f\"  Median Percentage Error: {pct_errors.median():.2f}%\")\n",
        "\n",
        "# Percentage of predictions within certain error margins\n",
        "within_10pct = (pct_errors <= 10).sum() / len(pct_errors) * 100\n",
        "within_20pct = (pct_errors <= 20).sum() / len(pct_errors) * 100\n",
        "within_30pct = (pct_errors <= 30).sum() / len(pct_errors) * 100\n",
        "\n",
        "print(f\"\\nPrediction Accuracy:\")\n",
        "print(f\"  Within 10% error: {within_10pct:.1f}%\")\n",
        "print(f\"  Within 20% error: {within_20pct:.1f}%\")\n",
        "print(f\"  Within 30% error: {within_30pct:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to CSV\n",
        "results_df.to_csv('model_comparison_results.csv', index=False)\n",
        "print(\"‚úì Results saved to 'model_comparison_results.csv'\")\n",
        "\n",
        "# Save best model predictions\n",
        "predictions_df = pd.DataFrame({\n",
        "    'Actual': y_test_original,\n",
        "    'Predicted': best_predictions,\n",
        "    'Error': y_test_original - best_predictions,\n",
        "    'Absolute_Error': np.abs(y_test_original - best_predictions),\n",
        "    'Percentage_Error': (np.abs(y_test_original - best_predictions) / y_test_original) * 100\n",
        "})\n",
        "predictions_df.to_csv(f'{best_model_name}_predictions.csv', index=False)\n",
        "print(f\"‚úì Best model predictions saved to '{best_model_name}_predictions.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Final Summary & Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FINAL SUMMARY & RECOMMENDATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nüèÜ BEST MODEL: {best_model_name.upper()}\")\n",
        "print(f\"\\nüìä Key Metrics:\")\n",
        "print(f\"   - RMSE: {best_model_metrics['RMSE']:.2f} units\")\n",
        "print(f\"   - MAE: {best_model_metrics['MAE']:.2f} units\")\n",
        "print(f\"   - R¬≤ Score: {best_model_metrics['R¬≤']:.4f} ({best_model_metrics['R¬≤']*100:.2f}% variance explained)\")\n",
        "print(f\"   - MAPE: {best_model_metrics['MAPE (%)']:.2f}%\")\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  Efficiency:\")\n",
        "print(f\"   - Training Time: {best_model_metrics['Training Time (s)']:.2f} seconds\")\n",
        "\n",
        "print(f\"\\nüìà Model Interpretation:\")\n",
        "if best_model_metrics['R¬≤'] >= 0.7:\n",
        "    print(\"   - Excellent model performance!\")\n",
        "elif best_model_metrics['R¬≤'] >= 0.5:\n",
        "    print(\"   - Good model performance\")\n",
        "elif best_model_metrics['R¬≤'] >= 0.3:\n",
        "    print(\"   - Moderate model performance - room for improvement\")\n",
        "else:\n",
        "    print(\"   - Model performance could be improved\")\n",
        "\n",
        "print(f\"\\nüí° Recommendations:\")\n",
        "print(\"   1. Use the best performing model for production predictions\")\n",
        "print(\"   2. Consider hyperparameter tuning for further improvement\")\n",
        "print(\"   3. Collect more features if possible (e.g., seasonality, competitor prices)\")\n",
        "print(\"   4. Monitor model performance over time and retrain periodically\")\n",
        "if best_model_name in model_metadata['tree_models']:\n",
        "    print(\"   5. Focus on the most important features identified for feature engineering\")\n",
        "\n",
        "print(f\"\\nüìÅ Generated Files:\")\n",
        "print(\"   - model_comparison.png - Visual comparison of all models\")\n",
        "print(f\"   - {best_model_name}_predictions.png - Best model visualization\")\n",
        "if best_model_name in model_metadata['tree_models']:\n",
        "    print(\"   - feature_importance.png - Feature importance analysis\")\n",
        "print(\"   - distribution_comparison.png - Distribution analysis\")\n",
        "print(\"   - model_comparison_results.csv - Detailed metrics table\")\n",
        "print(f\"   - {best_model_name}_predictions.csv - Prediction details\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ MODEL EVALUATION COMPLETE!\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This notebook has evaluated all trained models and identified the best performer for predicting `purchased_last_month`.\n",
        "\n",
        "### Next Steps (Optional):\n",
        "1. **Hyperparameter Tuning:** Use GridSearchCV or RandomizedSearchCV to optimize the best model\n",
        "2. **Feature Engineering:** Create additional features based on domain knowledge\n",
        "3. **Ensemble Methods:** Combine multiple models for potentially better performance\n",
        "4. **Production Deployment:** Save the best model and create a prediction pipeline\n",
        "5. **Model Monitoring:** Set up tracking to monitor model performance over time"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
